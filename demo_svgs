import torch
import torch.nn as nn
from tqdm import tqdm
import numpy as np
import torch.optim as optim
from captum.attr import IntegratedGradients
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

import os
import random
from torch.backends import cudnn

import scanpy as sc


def fix_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    cudnn.deterministic = True
    cudnn.benchmark = False

    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'


class MultiClassModel(nn.Module):
    def __init__(self, in_features, n_classes):
        super(MultiClassModel, self).__init__()
        self.layer1 = nn.Linear(in_features, 32)
        self.layer2 = nn.Linear(32, n_classes)
        # self.layer2 = nn.Linear(1000, 100)
        # self.layer3 = nn.Linear(100, n_classes)
        # self.relu1 = nn.ReLU()
        # self.relu2 = nn.ReLU()

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        # x = self.relu1(x)
        # x = self.layer2(x)
        # x = self.relu2(x)
        # x = self.layer3(x)
        return x


def multi_target_integrated_gradients(inputs, num_classes, model):
    print('multi_target_integrated_gradients start')
    ig_2 = IntegratedGradients(model)
    inputs_2 = inputs.clone()
    inputs_2.requires_grad_()
    baselines = torch.zeros_like(inputs_2)
    all_attributions = []
    # method='riemann_trapezoid'
    for c in range(num_classes):
        attributions = ig_2.attribute(inputs_2, baselines=baselines, target=c, n_steps=10)
        all_attributions.append(attributions.cpu())
    # 转换为tensor，shape: num_classes, num_samples, num_features)
    all_attributions = torch.stack(all_attributions, dim=0)
    important_gene_counts = (torch.zeros(inputs_2.shape[1])).to(device)
    for attributions in all_attributions:
        median_attributions, _ = attributions.median(dim=0)
        
        k = int(0.7 * len(median_attributions))
        topk_values, topk_indices = torch.topk(median_attributions, k)
        important_gene_counts[topk_indices] += 1

    threshold = num_classes * 0.7
    important_gene_indices = torch.nonzero(important_gene_counts >= threshold, as_tuple=True)[0]
    return important_gene_indices


def single_target_integrated_gradients(inputs, c, model, std_multiplier=1.5):
    print('single_target_integrated_gradients start ' + str(c))
    ig_1 = IntegratedGradients(model)
    inputs_1 = inputs.clone()
    # print('inputs_1.shape:', inputs_1.shape)
    inputs_1.requires_grad_()
    # print('n_sample:', n_sample)
    baselines = torch.zeros_like(inputs_1)
    # internal_batch_size=32

    # attributions = ig_1.attribute(inputs_1, baselines=baselines, target=c, internal_batch_size=n_sample)
    attributions = ig_1.attribute(inputs_1, baselines=baselines, target=c, n_steps=10)

    mask = (y_train == c)
    attributions_domain = attributions[mask]
      
    median_attributions, _ = attributions_domain.median(dim=0)
    positive_indices = torch.nonzero(median_attributions > 0, as_tuple=True)[0]
    positive_genes = median_attributions[positive_indices]

    mean_val = positive_genes.mean()
    std_val = positive_genes.std()
    adaptive_threshold = mean_val + std_multiplier * std_val
    important_gene_indices = positive_indices[positive_genes > adaptive_threshold]
    return important_gene_indices


from collections import Counter


def preprocess(adata):
    sc.pp.filter_genes(adata, min_cells=10)
    # sc.pp.filter_genes(adata, min_counts=10)
    # sc.pp.highly_variable_genes(adata, flavor="seurat_v3", n_top_genes=6000)
    # sc.pp.normalize_total(adata, target_sum=1e4)
    # sc.pp.log1p(adata)
    # sc.pp.scale(adata, zero_center=False, max_value=10)

if __name__ == '__main__':
    svg_plot = False
    if svg_plot == False:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # device ='cpu'
        file_fold = '/home/liangxiao/lllxxx/Human_Breast_Cancer/'
        adata = sc.read_visium(file_fold)
        # sc.pp.filter_genes(adata, min_cells=3)
        adata.var_names_make_unique()
        preprocess(adata)
        # adata = adata[:, adata.var['highly_variable']]

        de_emb = adata.X.toarray()
        # de_emb = np.load('HBC_res/de_emb.npy')
        print('de_emb shape:', de_emb.shape)
        new_type = np.load('HBC_res/new_type.npy')
        new_type = new_type.astype(int)
        new_type_int = new_type.copy()
        # gene_name = np.load('HBC_res/gene_name.npy', allow_pickle=True)
        gene_name = adata.var.index
        print('len(gene_name):', len(gene_name))
        # a = torch.tensor([1253, 10662])
        # print(gene_name[a])
        de_emb = torch.FloatTensor(de_emb).to(device)
        new_type = torch.tensor(new_type, dtype=torch.long).to(device)
        n_cluster = len(set(new_type_int))
        print('n_cluster:', n_cluster)

        fix_seed(2025)
        X = de_emb
        y = new_type
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=2025)
        # import numpy as np
        # y_train_int = y_train.cpu().numpy()
        # unique, counts = np.unique(y_train_int, return_counts=True)
        # total = counts.sum()
        # proportions = counts /  total
        # for cls, count, prop in zip(unique, counts, proportions):
        #     print(f"Class {cls}: Count = {count}, Proportion = {prop:.2%}")
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)

        mlp = MultiClassModel(de_emb.shape[1], n_cluster).to(device)
        optimizer = optim.AdamW(mlp.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()

        min_epochs = 100

        patience = 20
        delta = 0.0001
        best_loss = float('inf')
        early_stop_counter = 0
        # best_model_path = "mlp_res/best_mlp_model.pth"

        mlp.train()
        for epoch in range(300):
            mlp.train()
            for batch_X, batch_y in train_loader:
                outputs = mlp(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

            mlp.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = mlp(batch_X)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            val_loss /= len(val_loader)

            # print(val_loss)

            if epoch >= min_epochs:
                if val_loss < best_loss - delta:
                    best_loss = val_loss
                    early_stop_counter = 0
                    # torch.save(mlp.state_dict(), best_model_path)
                    # print(f"Validation loss improved to {val_loss:.4f}, saving model.")
                else:
                    early_stop_counter += 1
                    # print(f"No improvement in validation loss. Counter: {early_stop_counter}/{patience}")

                if early_stop_counter >= patience:
                    # print("Early stopping triggered. Training stopped.")
                    break

        mlp.eval()


        svg_list_s = []
        for c in range(n_cluster):
            svg_idx = single_target_integrated_gradients(X_train, c, mlp)
            svg_idx = svg_idx.cpu()
            # print(svg_idx)
            # print(list(gene_name[svg_idx]))
            try:
                svg_list_s.append(list(gene_name[svg_idx]))
            except:
                svg_list_s.append([gene_name[svg_idx.item()]])
            # print(svg_idx)
            # print(svg_list_s)

        # # 2  
        svg_idx_multi = multi_target_integrated_gradients(X_train, n_cluster, mlp)
        svg_idx_multi = svg_idx_multi.cpu()
        # print(svg_idx_multi)
        non_svg_list = list(gene_name[svg_idx_multi])
        # print(non_svg_list)
        print('no need genes:', len(non_svg_list))

        non_svg_list = []


        #  
        svg_list = [list(filter(lambda x: x not in non_svg_list, sublist)) for sublist in svg_list_s]

        # np.save('/home/liangxiao/lllxxx/SpaLJP/HBC_res/svg_res/spaLJP_svg_HBC_list_with_domain.npy', svg_list)

        # svg_list = svg_list_s
        #
        print(svg_list)

        flat_list = [item for sublist in svg_list for item in sublist]
