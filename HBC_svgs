import torch
import torch.nn as nn
import numpy as np
import torch.optim as optim
from captum.attr import IntegratedGradients
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import os
import random
from torch.backends import cudnn
import scanpy as sc


def fix_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    cudnn.deterministic = True
    cudnn.benchmark = False

    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'


class MultiClassModel(nn.Module):
    def __init__(self, in_features, n_classes):
        super(MultiClassModel, self).__init__()
        self.layer1 = nn.Linear(in_features, 32)
        self.layer2 = nn.Linear(32, n_classes)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        return x


def multi_target_integrated_gradients(inputs, num_classes, model):
    print('multi_target_integrated_gradients start')
    ig_2 = IntegratedGradients(model)
    inputs_2 = inputs.clone()
    inputs_2.requires_grad_()
    baselines = torch.zeros_like(inputs_2)
    all_attributions = []
    for c in range(num_classes):
        attributions = ig_2.attribute(inputs_2, baselines=baselines, target=c, n_steps=10)
        all_attributions.append(attributions.cpu())
    all_attributions = torch.stack(all_attributions, dim=0)
    important_gene_counts = (torch.zeros(inputs_2.shape[1])).to(device)
    for attributions in all_attributions:
        median_attributions, _ = attributions.median(dim=0)
        k = int(0.7 * len(median_attributions))
        topk_values, topk_indices = torch.topk(median_attributions, k)
        important_gene_counts[topk_indices] += 1
    threshold = num_classes * 0.7
    important_gene_indices = torch.nonzero(important_gene_counts >= threshold, as_tuple=True)[0]
    return important_gene_indices


def single_target_integrated_gradients(inputs, c, model, std_multiplier=1.5):
    print('single_target_integrated_gradients start ' + str(c))
    ig_1 = IntegratedGradients(model)
    inputs_1 = inputs.clone()
    # print('inputs_1.shape:', inputs_1.shape)
    inputs_1.requires_grad_()
    # print('n_sample:', n_sample)
    baselines = torch.zeros_like(inputs_1)
    # 1
    attributions = ig_1.attribute(inputs_1, baselines=baselines, target=c, n_steps=10)
    # 2
    mask = (y_train == c)
    attributions_domain = attributions[mask]
    # 3
    median_attributions, _ = attributions_domain.median(dim=0)
    positive_indices = torch.nonzero(median_attributions > 0, as_tuple=True)[0]
    positive_genes = median_attributions[positive_indices]
    # 4
    mean_val = positive_genes.mean()
    std_val = positive_genes.std()
    adaptive_threshold = mean_val + std_multiplier * std_val
    important_gene_indices = positive_indices[positive_genes > adaptive_threshold]
    return important_gene_indices


def preprocess(adata):
    sc.pp.filter_genes(adata, min_cells=10)

if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    file_fold = '/home/liangxiao/lllxxx/Human_Breast_Cancer/'
    adata = sc.read_visium(file_fold)
    adata.var_names_make_unique()
    preprocess(adata)

    de_emb = adata.X.toarray()
    print('de_emb shape:', de_emb.shape)
    new_type = np.load('HBC_res/new_type.npy')
    new_type = new_type.astype(int)
    new_type_int = new_type.copy()
    gene_name = adata.var.index
    print('len(gene_name):', len(gene_name))
    de_emb = torch.FloatTensor(de_emb).to(device)
    new_type = torch.tensor(new_type, dtype=torch.long).to(device)
    n_cluster = len(set(new_type_int))
    print('n_cluster:', n_cluster)

    fix_seed(2025)
    X = de_emb
    y = new_type
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=2025)
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)

    mlp = MultiClassModel(de_emb.shape[1], n_cluster).to(device)
    optimizer = optim.AdamW(mlp.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    min_epochs = 100

    patience = 20
    delta = 0.0001
    best_loss = float('inf')
    early_stop_counter = 0

    mlp.train()
    for epoch in range(300):
        mlp.train()
        for batch_X, batch_y in train_loader:
            outputs = mlp(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        mlp.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                outputs = mlp(batch_X)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
        val_loss /= len(val_loader)

        if epoch >= min_epochs:
            if val_loss < best_loss - delta:
                best_loss = val_loss
                early_stop_counter = 0
            else:
                early_stop_counter += 1

            if early_stop_counter >= patience:
                break

    mlp.eval()

    svg_list_s = []
    for c in range(n_cluster):
        svg_idx = single_target_integrated_gradients(X_train, c, mlp)
        svg_idx = svg_idx.cpu()
        try:
            svg_list_s.append(list(gene_name[svg_idx]))
        except:
            svg_list_s.append([gene_name[svg_idx.item()]])

    # # 2
    svg_idx_multi = multi_target_integrated_gradients(X_train, n_cluster, mlp)
    svg_idx_multi = svg_idx_multi.cpu()
    # print(svg_idx_multi)
    non_svg_list = list(gene_name[svg_idx_multi])
    # print(non_svg_list)
    print('no need genes:', len(non_svg_list))

    non_svg_list = []


    svg_list = [list(filter(lambda x: x not in non_svg_list, sublist)) for sublist in svg_list_s]

    # np.save('/home/liangxiao/lllxxx/SpaLJP/HBC_res/svg_res/spaLJP_svg_HBC_list_with_domain.npy', svg_list)
